# LLM API Configuration for RAG Chat
# Copy this file to .env and fill in your actual values

# =============================================================================
# LLM Provider Selection
# =============================================================================
# Choose which LLM provider to use. Options: custom, openai, anthropic, ollama
# Default: custom (backwards compatible with existing HMAC-authenticated API)
LLM_PROVIDER=custom

# =============================================================================
# Custom HMAC-Authenticated API (default provider)
# =============================================================================
# Your API key for authentication
API_KEY=your_api_key_here

# Your API secret for HMAC signature
API_SECRET=your_api_secret_here

# Base URL for the LLM API endpoint (without trailing slash)
BASE_URL=https://your-llm-api-endpoint.com

# =============================================================================
# OpenAI Configuration (when LLM_PROVIDER=openai)
# =============================================================================
# Your OpenAI API key (starts with sk-)
OPENAI_API_KEY=sk-your-api-key-here

# Model to use (default: gpt-4o)
# Options: gpt-4o, gpt-4-turbo, gpt-4, gpt-3.5-turbo, etc.
OPENAI_MODEL=gpt-4o

# =============================================================================
# Anthropic Configuration (when LLM_PROVIDER=anthropic)
# =============================================================================
# Your Anthropic API key (starts with sk-ant-)
ANTHROPIC_API_KEY=sk-ant-your-api-key-here

# Model to use (default: claude-3-5-sonnet-20241022)
# Options: claude-3-5-sonnet-20241022, claude-3-opus-20240229, claude-3-haiku-20240307
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# =============================================================================
# Ollama Configuration (when LLM_PROVIDER=ollama)
# =============================================================================
# For local/offline use with Ollama (https://ollama.ai/)

# Base URL for Ollama server (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Model to use (default: llama3.2)
# Run 'ollama list' to see available models
OLLAMA_MODEL=llama3.2
